version: "3.9"

networks:
  airgap:
    internal: true  # blocks egress; services only see each other

volumes:
  pg_data:

services:
  db:
    image: pgvector/pgvector:pg16
    container_name: rag_db
    environment:
      POSTGRES_USER: rag
      POSTGRES_PASSWORD: ragpw
      POSTGRES_DB: rag
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rag -d rag"]
      interval: 5s
      timeout: 3s
      retries: 20
    volumes:
      - pg_data:/var/lib/postgresql/data
      - ./db/init.sql:/docker-entrypoint-initdb.d/00_init.sql:ro
    networks: [airgap]
    restart: unless-stopped
    read_only: false
    cap_drop: [ALL]
    security_opt: ["no-new-privileges:true"]

  llm:
    image: vllm/vllm-openai:latest
    container_name: rag_llm
    command: >
      --model /models/llm
      --served-model-name main
      --dtype auto
      --max-model-len 8192
      --port 8000
    environment:
      VLLM_ALLOW_RUNTIME_DOWNLOADS: "0"
    ports: ["8000:8000"]  # remove if you don't want host access
    volumes:
      - ./models/llm:/models/llm:ro  # put your LLM here (e.g., Qwen2.5-7B-Instruct)
    networks: [airgap]
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    device_requests:
      - driver: nvidia
        count: -1
        capabilities: [["gpu"]]
    restart: unless-stopped
    read_only: true
    cap_drop: [ALL]
    security_opt: ["no-new-privileges:true"]

  app:
    build:
      context: ./app
      dockerfile: Dockerfile
    container_name: rag_api
    env_file: [.env]
    depends_on:
      db: { condition: service_healthy }
      llm: { condition: service_started }
    volumes:
      - ./data:/data:ro        # your documents to index
      - ./models/emb:/models/emb:ro  # bge model folder
    networks: [airgap]
    ports: ["7000:7000"]  # remove if you don't want host access
    restart: unless-stopped
    read_only: true
    cap_drop: [ALL]
    security_opt: ["no-new-privileges:true"]
